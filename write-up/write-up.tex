\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\title{RoadRunner}

\author{Pedro Cattori and Brando Miranda}

\begin{document}
\maketitle

\begin{abstract}
RoadRunner is a persistent, fault-tolerant, high-performance key-value store. 
RoadRunner avoids two round trip times by preparing everything in the Paxos log in advance from some sequence number to infinity. 
Our implementation of Multi-Paxos is independent of clock synchronization or time leases.
Our design of Multi-Paxos does not need to elect leaders explicitly by using Paxos instance to decide on an server id number.
Instead, from the servers that are alive, the one with the highest server id will consider himself the leader and prepare Paxos instances in advance (and eventually send accepts too).
The correctness of our Multi-Paxos design will not be affected if multiple servers think they are the leader. 
In fact, if the leader is unstable and the leader switches around constantly during the operation of RoadRunner, it will degrade gracefully into normal Paxos. 
Thus, in the worst-case where RoadRunner's servers die constantly, it will offer performance at least as good as normal Paxos.

\indent For persistence, Multi-Paxos will only reply to the proposers once it has persistence the acceptors on disk. This is to not change already decided values. 
RoadRunner also writes the key-value store to disk and persists the local min, so that it knows what operations from the Multi-Paxos log to not re-apply. 
Furthermore, if one server has its disk contents crash, we have a mechanism to ensure correctness of the whole service when we add back a functional server to the system.
\end{abstract}

\section{Protocol Design}

Conceptually, the reason for this is because the effective leader will send the highest epoch round along with any prepares or accepts he does, so two things will happen; first, old leader will have a lower epoch round so servers will reject old accepts, second, new leader will always prepare before sending accepts, so they will learn of old decided value if there are any. 


\subsection{Overview}

\subsubsection{Overview of Multi-Paxos}

In the case of a stable leader Multi-Paxos will have a performance benefit by avoiding round trip times when starting Paxos instances in the log.
Multi-Paxos will prepare all sequences in the Paxos log from a given sequence number to infinity with the current round number.
However, what used to be a round number in normal Paxos will have different semantic in our Multi-Paxos.
In our design, a round number will still be used to reject old accepts and prepare's, however, they will also correspond to the epoch round of the current leader. 
Therefore, round numbers will be referred to as epoch numbers and the current acting leader will have the highest epoch round number (corresponding to the round in which he is the leader).
Therefore, the leader will send prepare and accept messages with his epoch number and, if any other old leader tries to send accept or prepare messages with a lower epoch number, servers that have already been prepared with the current leader will reject and inform him that he should find out who the real leader is.
Therefore, we ensure that old leader cannot change any already decide values because if a majority of the new servers have been prepared by the new leader, they will reject any old accept or prepare message and inform the confused leader about the new leader (because the majority was prepared with a higher epoch number).
Furthermore, new leader cannot possibly change old decided values either. 
This is because when a new leader is formed, he will *always* send prepare messages before sending accept messages.
Therefore, whenever he tries propose any new values, he will always be learn a decided value by some majority, if one exists.
\\
\indent However, how do we make sure that the system agrees on who the leader is? 
This is enforced through our periodic ticks.
All the servers will ping each other and if one of the servers does not respond for a long enough time, a server will consider it dead.
If the dead server was the leader, then the servers will learn that a new leader has to be formed.
From the pings responses, he knows which  servers are aline and which are not.
Whoever has the highest server id should act as the leader and that is how servers know who the new leader is.
If a client needs to know who the leader is, he can be informed by any server of who they believe the leader is and the client can eventually route requests to the true leader.
Notice that it is possible that in a really bad network situation or if there is no stable leader, that the servers will duel to be the leader.
In that case, then that case servers will prepare and increasing their epoch number and inevitably duel for Paxos instances.
A similar scenario was possible normal in Paxos, so in a temporary bad scenario, the system will  behave like normal Paxos.
\\
\indent Our Multi-Paxos library can also handle disk crashes.
The way that it ensures correct Paxos behaviour is by persisting the acceptors before replying to a proposer from the leader.
The reason that this is important is because, when a acceptors replies to a proposer, it promises that it will not accept values with a epoch number lower than n.
If that is true, but then acceptor forgets that it did that promise, it is possible that a decided value that was "locked", reverses, which is bad behaviour in Paxos.
Thus, for the correct behaviour of the Paxos, we persist acceptors.

\subsubsection{Overview of the whole Service and Persistance}

RoadRunner is a sharded key-value store (that garbage collects), which means that our system is able to handle very large amounts of data.
Our system is made fault-tolerent through Multi-Paxos and disk writes.
The things that we write to disk are: key-value store, the duplicate detection table (history of keys and shards), the Multi-Paxos acceptors and the local min (one more than the last sequence number that was garbage collected, i.e. Done was called on it).
If one server crashes with its disk intact, then it can return to its normal operation by reading the disk.
The key-value store he stored will make sure that he doesn't have to re-apply every operation ever done to its key-vale store to get it up to date.
The fact that he knows the local min means that he knows from which operations he needs to learn decided value and which ones his key-value store might be behind. 
Remembering the history of the shards makes sure that duplicate detection works normal when he comes back up.
Remembering the acceptors  ensures the correct behaviour of multi-Paxos.\\
\indent If the disk crashes too, one has to be more careful on how the system proceeds.
The reason is because we have lost all the acceptors for any sequence in the Paxos log.
Therefore, before the system resumes operation, it needs to make sure that it does not reverse any values that had previously been  decided (or worse, not reverse any values that were already applied to the key-value store).
Conceptually, it would be nice if we could copy the state of some server such that we could get back the acceptors when we formed any decision.
This is equivalent to knowing the sequence number of the highest decision we have contributed to. 
In turn, the highest decision we have contributed to is less than or to the sequence number of the highest decision ever made.
Therefore, if we could go to a state when the highest decision was applied to the key-value store, then we would remove the risk of reverting anything. 
Thus, its enough to guarantee correctness that our service recovers from disk by pinging all the servers that are alive and finding out the



\end{document}